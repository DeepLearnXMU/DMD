{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def cal_metrics(labels, answers):\n",
    "    accuracy = accuracy_score(labels, answers)\n",
    "    precision = precision_score(labels, answers)\n",
    "    recall = recall_score(labels, answers)\n",
    "    f1 = f1_score(labels, answers)\n",
    "\n",
    "    return {\"acc\": accuracy, \"prec\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def load_data(path, train_dataset=None, train=False):\n",
    "    def remove_non_letters(s):\n",
    "        return re.sub(r\"^[^a-zA-Z]+|[^a-zA-Z]+$\", \"\", s)\n",
    "\n",
    "    with open(\"../data/dictionary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        dictionary = json.load(f)\n",
    "\n",
    "    with open(\"../data/contexts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        contexts = json.load(f)\n",
    "\n",
    "    if train_dataset is not None:\n",
    "        with open(os.path.join(path, \"distances.json\"), \"r\") as f:\n",
    "            distances = json.load(f)\n",
    "\n",
    "        with open(os.path.join(path, \"vals.json\"), \"r\") as f:\n",
    "            vals = json.load(f)\n",
    "\n",
    "    if train:\n",
    "        data_path = os.path.join(path, \"train.tsv\")\n",
    "    else:\n",
    "        data_path = os.path.join(path, \"test.tsv\")\n",
    "\n",
    "    labels = []\n",
    "    dataset = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for index, line in enumerate(lines):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        cells = line.strip().split(\"\\t\")\n",
    "\n",
    "        label = int(cells[1])\n",
    "        sent = cells[2]\n",
    "        POS = cells[3]\n",
    "        v_index = int(cells[-1])\n",
    "        word = remove_non_letters(sent.split()[v_index])\n",
    "\n",
    "        splits = sent.split()\n",
    "        splits.insert(v_index, \"<tar>\")\n",
    "        splits.insert(v_index + 2, \"</tar>\")\n",
    "\n",
    "        sample = {\"sentence\": sent, \"word\": word, \"label\": label, \"pos\": POS, \"v_index\": v_index, \"s_sentence\": \" \".join(splits)}\n",
    "        if train_dataset is not None:\n",
    "            shots = random.sample(train_dataset, 10)\n",
    "\n",
    "            sample[\"shots\"] = shots\n",
    "        if train_dataset is not None:\n",
    "            samples_distances = distances[index - 1]\n",
    "            samples_ids = vals[index - 1]\n",
    "            sample[\"samples_distances\"] = samples_distances\n",
    "            sample[\"samples_knn\"] = [train_dataset[_id] for _id in samples_ids]\n",
    "        word = word.lower()\n",
    "        base_words = [word]\n",
    "        for pos in [\"v\", \"a\", \"r\", \"s\", \"n\"]:\n",
    "            base_word = lemmatizer.lemmatize(word, pos)\n",
    "            if base_word in base_words:\n",
    "                continue\n",
    "            base_words.append(base_word)\n",
    "\n",
    "        for word in base_words:\n",
    "            if word in dictionary:\n",
    "                dict_info = dictionary[word.lower()]\n",
    "                sample[\"dict_word\"] = word\n",
    "                sample[\"dict\"] = dict_info\n",
    "                break\n",
    "            else:\n",
    "                sample[\"dict\"] = {}\n",
    "\n",
    "        for word in base_words:\n",
    "            if word in contexts:\n",
    "                sample[\"pos_sent\"] = contexts[word][\"pos\"][0]\n",
    "                sample[\"neg_sent\"] = contexts[word][\"neg\"][0]\n",
    "                sample[\"exam_word\"] = word\n",
    "                break\n",
    "\n",
    "        dataset.append(sample)\n",
    "        labels.append(int(cells[1]))\n",
    "\n",
    "    return dataset, labels\n",
    "train_dataset, _ = load_data(\"../data/VUA18\", train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "llm_type = \"gpt-4o-2024-08-06\"\n",
    "api_key = \"\"\n",
    "api_base = \"\"\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "global_vars = {\"EXAMNUM\": 2}\n",
    "\n",
    "\n",
    "def get_response(llm_type, prompt, temp=1.0):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_type,\n",
    "        stream=False,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temp,\n",
    "        timeout=600,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def load_prompts(dataset, func):\n",
    "    res = []\n",
    "    for data in dataset:\n",
    "        res.append(func(data))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def prompt_func(data):\n",
    "    prompt_template = \"\"\"## Backgroud\n",
    "\n",
    "A metaphor is a rhetorical device that makes a non-literal comparison between two unlike things.\n",
    "\n",
    "There are different types of metaphors:\n",
    "- Direct Metaphor: Explicitly compares two unrelated things by stating that one thing is another, usually using a form of the verb \"be\".\n",
    "- Implied Metaphor: Compares two unlike things without explicitly naming one of them, often using a non-literal verb.\n",
    "- Extended Metaphor: Develops an initial comparison over several lines or paragraphs.\n",
    "- Mixed Metaphor: Combines two or more metaphors, often resulting in a confusing or nonsensical statement, usually unintentionally humorous.\n",
    "- Dead Metaphor: A metaphor that has become so familiar due to repeated use that people no longer recognize it as a metaphor, understanding it as having a straightforward meaning.\n",
    "\n",
    "The MIP (Metaphor Identification Procedure) theory and the SPV (Selectional Preference Violation) theory are two theories that determine whether a word expresses metaphor in a sentence.\n",
    "\n",
    "### Metaphor Identification Procedure\n",
    "\n",
    "MIP is a systematic method used to identify metaphorical expressions in text. The core idea is to compare the literal meaning of a word with its contextual meaning to determine if a word is metaphorical. \n",
    "Here are the specific steps of MIP:\n",
    "1. Select the text to analyze: Choose a segment of text or a sentence.\n",
    "2. Tokenization and annotation: Break down the text into individual words and annotate each word with its basic (literal) meaning.\n",
    "3. Compare literal and contextual meanings: For each word, determine if its meaning in the current context differs from its common literal meaning.\n",
    "4. Identify metaphorical usage: If the contextual meaning of a word is significantly different from its literal meaning and this difference is understood metaphorically, then the word is identified as metaphorical.\n",
    "\n",
    "### Selectional Preference Violation\n",
    "\n",
    "SPV is a method used to identify metaphors by analyzing the anomaly of a target word in the context of its surrounding words. The core idea is that metaphorical words often violate their selectional preferences in a given context.\n",
    "The specific steps of SPV are as follows:\n",
    "1. Identify the target word and its surrounding words: Choose a target word and analyze its surrounding words (usually adjacent words or phrases).\n",
    "2. Analyze selectional preferences: Determine the normal selectional preferences of the target word, i.e., the types of words it typically appears with in a given context.\n",
    "3. Detect anomalies: Assess whether the usage of the target word in the current context violates its normal selectional preferences. If the target word's usage appears unusual or does not conform to its typical selectional preferences, it is likely to be metaphorical.\n",
    "\n",
    "## Instruction\n",
    "\n",
    "Please use the following step-by-step instructions to check if the given word expresses metaphorical in the given sentence:\n",
    "1. Identify the Target Word:\n",
    " - Locate the target word in the provided sentence.\n",
    "2. Contextual Meaning (SPV Theory):\n",
    " - Analyze the sentence to understand the contextual meaning of the target word.\n",
    " - Determine if the target word is used to describe a source domain (the literal meaning) and a target domain (the metaphorical meaning).\n",
    "3. Basic Meaning (MIP Theory):\n",
    " - Determine the most basic meaning of the target word that is more concrete, related to bodily action, more precise, and historically older.\n",
    " - Check if this basic meaning contrasts with the contextual meaning identified in step 2.\n",
    "4. Check for Metaphorical Usage (MIP Theory):\n",
    " - If there is a contrast between the basic meaning and the contextual meaning, consider if the contextual meaning can be understood in terms of the basic meaning (i.e., the target word is used metaphorically).\n",
    "5. SPV Analysis:\n",
    " - Determine if the target word involves personification (assigning human characteristics to non-human entities) or if it acts as a vehicle (metaphorical term that carries meaning from the source to the target).\n",
    "6. Final Determination:\n",
    " - Based on the analysis, conclude whether the target word is used metaphorically in the given sentence.\n",
    "7. Answer:\n",
    " - Select final answer from 'yes' or 'no'.\n",
    "\n",
    "## Input\n",
    "\n",
    "- [sentence] {sentence}\n",
    "- [word] {word}\n",
    "- [dictionary] {dict_info}\n",
    "\"\"\"\n",
    "\n",
    "    def get_dict_prompt(data):\n",
    "        if \"dict_word\" not in data or \"dict\" not in data:\n",
    "            return \"\"\n",
    "\n",
    "        dict_template = \"\"\"{word}:\"\"\".format(word=data[\"dict_word\"])\n",
    "\n",
    "        dict_prompt = \"\"\n",
    "        index = 1\n",
    "\n",
    "        for dict_info in data[\"dict\"]:\n",
    "            try:\n",
    "                key = dict_info[\"definition\"]\n",
    "                item = dict_info[\"examples\"][0]\n",
    "                if not key or not item:\n",
    "                    continue\n",
    "                dict_prompt += \"\"\"definition {index}: {key}\n",
    "    - example: {item}\n",
    "\"\"\".format(\n",
    "                    key=dict_info[\"definition\"], index=index, item=dict_info[\"examples\"][0]\n",
    "                )\n",
    "                index += 1\n",
    "                if index == 3:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return dict_template + \"\\n\" + dict_prompt.rstrip()\n",
    "\n",
    "    return prompt_template.format(sentence=data[\"sentence\"], word=data[\"word\"], dict_info=get_dict_prompt(data))\n",
    "\n",
    "\n",
    "for times in [1, 2, 3]:\n",
    "    for data_name in [\n",
    "        \"MOH-X\",\n",
    "        \"TroFi\",\n",
    "    ]:\n",
    "\n",
    "        setting = f\"explicit-{times}\"\n",
    "\n",
    "        data_dir = f\"../data/EVAL-samples/{data_name}\"\n",
    "\n",
    "        output_dir = f\"./results/{setting}/{data_name}/{llm_type}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"===================={output_dir}=====================\")\n",
    "\n",
    "        dataset, labels = load_data(data_dir, train_dataset=train_dataset)\n",
    "\n",
    "        prompts = load_prompts(dataset, prompt_func)\n",
    "\n",
    "        with open(os.path.join(output_dir, \"prompts.json\"), \"w\") as f:\n",
    "            json.dump(prompts, f)\n",
    "        while True:\n",
    "            try:\n",
    "                with open(os.path.join(output_dir, \"responses.json\"), \"r\") as f:\n",
    "                    responses = json.load(f)\n",
    "            except:\n",
    "                responses = []\n",
    "\n",
    "            if len(dataset) == len(responses):\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                for index, prompt in tqdm(enumerate(prompts[len(responses) :]), total=len(prompts) - len(responses)):\n",
    "                    response = get_response(llm_type, prompt, temp=0)\n",
    "                    responses.append(response)\n",
    "                    if index % 50 == 0:\n",
    "                        with open(os.path.join(output_dir, \"responses.json\"), \"w\") as f:\n",
    "                            json.dump(responses, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(os.path.join(output_dir, \"responses.json\"), \"w\") as f:\n",
    "                    json.dump(responses, f)\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            with open(os.path.join(output_dir, \"responses.json\"), \"w\") as f:\n",
    "                json.dump(responses, f)\n",
    "\n",
    "        def extract_answer(response):\n",
    "            match = re.search(r\"Answer: (yes|no)\", response, re.IGNORECASE)\n",
    "            try:\n",
    "                answer = match.group(1).lower()\n",
    "                if \"yes\" in answer:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            except Exception as e:\n",
    "                answer = 1 if response.lower().rfind(\"yes\") > response.lower().rfind(\"no\") else 0\n",
    "            return answer\n",
    "\n",
    "        preds = [extract_answer(response) for response in responses]\n",
    "        with open(os.path.join(output_dir, \"preds.json\"), \"w\") as f:\n",
    "            json.dump(preds, f)\n",
    "        metrics = cal_metrics(labels, preds)\n",
    "        with open(os.path.join(output_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
